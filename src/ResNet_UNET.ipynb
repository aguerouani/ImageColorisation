{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W3STlM4a-1PN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from skimage.color import rgb2lab, lab2rgb\n",
        "from torch import nn\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import random_split\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "L62kmgmphxTv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lab_to_rgb(L, ab):\n",
        "    \"\"\"\n",
        "    L is (1,H,W), ab is (2,H,W), both torch tensors.\n",
        "    L is in [0,1], ab is in [-1,1].\n",
        "    We convert them back to LAB then RGB.\n",
        "    \"\"\"\n",
        "    L = L[0].cpu().numpy()          # (H,W)\n",
        "    ab = ab.cpu().numpy().transpose(1,2,0)  # (H,W,2)\n",
        "\n",
        "    # Undo normalization\n",
        "    L = L * 100\n",
        "    ab = ab * 128\n",
        "\n",
        "    lab = np.concatenate([L[..., np.newaxis], ab], axis=2)  # (H,W,3)\n",
        "    rgb = np.clip(lab2rgb(lab.astype(np.float64)), 0, 1)\n",
        "    return rgb"
      ],
      "metadata": {
        "id": "PTRIRu89hzSn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "donCwSlT-bGe"
      },
      "outputs": [],
      "source": [
        "class RGB2LabDataset(Dataset):\n",
        "    def __init__(self, image_dir, image_size=256, extensions=('.jpg','.jpeg','.png','.bmp','.webp')):\n",
        "        self.image_paths = [\n",
        "            os.path.join(image_dir, f)\n",
        "            for f in os.listdir(image_dir)\n",
        "            if f.lower().endswith(extensions)\n",
        "        ]\n",
        "        if len(self.image_paths) == 0:\n",
        "            raise RuntimeError(f\"No images found in {image_dir}\")\n",
        "        self.transform = T.Compose([\n",
        "            T.Resize((image_size, image_size)),\n",
        "            T.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self): return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        x = self.transform(img)\n",
        "        x_np = x.permute(1,2,0).numpy().astype(np.float32)\n",
        "        lab = rgb2lab(x_np).astype(\"float32\")\n",
        "\n",
        "        L  = lab[...,0] / 100.0\n",
        "        ab = lab[...,1:] / 128.0\n",
        "\n",
        "        L  = torch.from_numpy(L).unsqueeze(0)\n",
        "        ab = torch.from_numpy(ab).permute(2,0,1)\n",
        "        return L, ab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet_UNet_AE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # -----------------------------------\n",
        "        # RESNET34 BACKBONE (FROZEN)\n",
        "        # -----------------------------------\n",
        "        resnet = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
        "        self.resnet_conv = nn.Sequential(*list(resnet.children())[:-2])  # → (B,512,H/32,W/32)\n",
        "\n",
        "        for p in self.resnet_conv.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        # -----------------------------------\n",
        "        # UNET ENCODER\n",
        "        # -----------------------------------\n",
        "        self.enc1 = self.block(1, 32)      # H\n",
        "        self.enc2 = self.block(32, 64)     # H/2\n",
        "        self.enc3 = self.block(64, 128)    # H/4\n",
        "        self.enc4 = self.block(128, 256)   # H/8\n",
        "        self.enc5 = self.block(256, 512)   # H/16\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "        # -----------------------------------\n",
        "        # BOTTLENECK FUSION\n",
        "        # (ResNet bottleneck + UNet bottleneck)\n",
        "        # -----------------------------------\n",
        "        self.fuse = nn.Conv2d(512 + 512, 512, kernel_size=1)\n",
        "\n",
        "        # -----------------------------------\n",
        "        # UNET DECODER (mirrors encoder)\n",
        "        # -----------------------------------\n",
        "        self.up5 = self.up_block(512, 512)    # H/16, matches e5\n",
        "        self.up4 = self.up_block(512, 256)     # H/8, matches e4\n",
        "        self.up3 = self.up_block(256, 128)     # H/4, matches e3\n",
        "        self.up2 = self.up_block(128, 64)      # H/2, matches e2\n",
        "        self.up1 = self.up_block(64, 32)       # H,   matches e1\n",
        "\n",
        "        # FINAL OUTPUT LAYER\n",
        "        self.out_conv = nn.Conv2d(32, 1, kernel_size=1)\n",
        "\n",
        "\n",
        "    # -----------------------------\n",
        "    # CONV BLOCK\n",
        "    # -----------------------------\n",
        "    def block(self, in_ch, out_ch):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "\n",
        "    # -----------------------------\n",
        "    # UPSAMPLING BLOCK\n",
        "    # -----------------------------\n",
        "    def up_block(self, in_ch, out_ch):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "\n",
        "    # -----------------------------\n",
        "    # FORWARD PASS\n",
        "    # -----------------------------\n",
        "    def forward(self, x):\n",
        "\n",
        "        # -----------------------------\n",
        "        # UNET ENCODER\n",
        "        # -----------------------------\n",
        "        e1 = self.enc1(x)               # H\n",
        "        e2 = self.enc2(self.pool(e1))   # H/2\n",
        "        e3 = self.enc3(self.pool(e2))   # H/4\n",
        "        e4 = self.enc4(self.pool(e3))   # H/8\n",
        "        e5 = self.enc5(self.pool(e4))   # H/16\n",
        "        e6 = self.pool(e5)              # H/32  (UNet bottleneck)\n",
        "\n",
        "        # -----------------------------\n",
        "        # RESNET ENCODER (frozen)\n",
        "        # -----------------------------\n",
        "        x_res = x.repeat(1, 3, 1, 1)    # convert 1-channel → fake RGB\n",
        "        res = self.resnet_conv(x_res)   # (B,512,H/32,W/32)\n",
        "\n",
        "        # -----------------------------\n",
        "        # BOTTLENECK FUSION\n",
        "        # -----------------------------\n",
        "        fused = torch.cat([e6, res], dim=1)  # (B,1024,H/32,W/32)\n",
        "        bottleneck = self.fuse(fused)        # (B,512,H/32,W/32)\n",
        "\n",
        "        # -----------------------------\n",
        "        # DECODER WITH SKIPS\n",
        "        # -----------------------------\n",
        "        d5 = self.up5(bottleneck)    # H/16\n",
        "        d5 = d5 + e5                 # skip\n",
        "\n",
        "        d4 = self.up4(d5)            # H/8\n",
        "        d4 = d4 + e4\n",
        "\n",
        "        d3 = self.up3(d4)            # H/4\n",
        "        d3 = d3 + e3\n",
        "\n",
        "        d2 = self.up2(d3)            # H/2\n",
        "        d2 = d2 + e2\n",
        "\n",
        "        d1 = self.up1(d2)            # H\n",
        "        d1 = d1 + e1\n",
        "\n",
        "        # final conv\n",
        "        out = self.out_conv(d1)      # (B,1,H,W)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "cgq0pyz_-5eC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6ce5029c"
      },
      "outputs": [],
      "source": [
        "def train(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for L, ab in loader:\n",
        "        L = L.to(device)\n",
        "        ab = ab.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(L)\n",
        "        loss = criterion(outputs, ab)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for L, ab in loader:\n",
        "            L = L.to(device)\n",
        "            ab = ab.to(device)\n",
        "            outputs = model(L)\n",
        "            loss = criterion(outputs, ab)\n",
        "            running_loss += loss.item()\n",
        "    return running_loss / len(loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyRKp5C0_967",
        "outputId": "f4320b4e-50c0-437e-a123-b2bb72877c61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = RGB2LabDataset(\".\", image_size=256)\n",
        "\n",
        "# Split dataset into training and evaluation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "eval_size = len(dataset) - train_size\n",
        "train_dataset, eval_dataset = random_split(dataset, [train_size, eval_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "lPfId_Q_ihPi"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "\n",
        "resnet = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
        "resnet_conv = nn.Sequential(*list(resnet.children())[:-2])\n",
        "\n",
        "for p in resnet_conv.parameters():\n",
        "    p.requires_grad = False"
      ],
      "metadata": {
        "id": "ndh99DFzi_Hj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet_UNet_AE().to(device)\n",
        "criterion = nn.SmoothL1Loss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "summary(model, input_size=(1, 256, 256), device=str(device))"
      ],
      "metadata": {
        "id": "Z46YcfVINOxe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c192db0c-05e7-48a0-b906-556bac1f721a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 32, 256, 256]             320\n",
            "       BatchNorm2d-2         [-1, 32, 256, 256]              64\n",
            "              ReLU-3         [-1, 32, 256, 256]               0\n",
            "            Conv2d-4         [-1, 32, 256, 256]           9,248\n",
            "       BatchNorm2d-5         [-1, 32, 256, 256]              64\n",
            "              ReLU-6         [-1, 32, 256, 256]               0\n",
            "         MaxPool2d-7         [-1, 32, 128, 128]               0\n",
            "            Conv2d-8         [-1, 64, 128, 128]          18,496\n",
            "       BatchNorm2d-9         [-1, 64, 128, 128]             128\n",
            "             ReLU-10         [-1, 64, 128, 128]               0\n",
            "           Conv2d-11         [-1, 64, 128, 128]          36,928\n",
            "      BatchNorm2d-12         [-1, 64, 128, 128]             128\n",
            "             ReLU-13         [-1, 64, 128, 128]               0\n",
            "        MaxPool2d-14           [-1, 64, 64, 64]               0\n",
            "           Conv2d-15          [-1, 128, 64, 64]          73,856\n",
            "      BatchNorm2d-16          [-1, 128, 64, 64]             256\n",
            "             ReLU-17          [-1, 128, 64, 64]               0\n",
            "           Conv2d-18          [-1, 128, 64, 64]         147,584\n",
            "      BatchNorm2d-19          [-1, 128, 64, 64]             256\n",
            "             ReLU-20          [-1, 128, 64, 64]               0\n",
            "        MaxPool2d-21          [-1, 128, 32, 32]               0\n",
            "           Conv2d-22          [-1, 256, 32, 32]         295,168\n",
            "      BatchNorm2d-23          [-1, 256, 32, 32]             512\n",
            "             ReLU-24          [-1, 256, 32, 32]               0\n",
            "           Conv2d-25          [-1, 256, 32, 32]         590,080\n",
            "      BatchNorm2d-26          [-1, 256, 32, 32]             512\n",
            "             ReLU-27          [-1, 256, 32, 32]               0\n",
            "        MaxPool2d-28          [-1, 256, 16, 16]               0\n",
            "           Conv2d-29          [-1, 512, 16, 16]       1,180,160\n",
            "      BatchNorm2d-30          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-31          [-1, 512, 16, 16]               0\n",
            "           Conv2d-32          [-1, 512, 16, 16]       2,359,808\n",
            "      BatchNorm2d-33          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-34          [-1, 512, 16, 16]               0\n",
            "        MaxPool2d-35            [-1, 512, 8, 8]               0\n",
            "           Conv2d-36         [-1, 64, 128, 128]           9,408\n",
            "      BatchNorm2d-37         [-1, 64, 128, 128]             128\n",
            "             ReLU-38         [-1, 64, 128, 128]               0\n",
            "        MaxPool2d-39           [-1, 64, 64, 64]               0\n",
            "           Conv2d-40           [-1, 64, 64, 64]          36,864\n",
            "      BatchNorm2d-41           [-1, 64, 64, 64]             128\n",
            "             ReLU-42           [-1, 64, 64, 64]               0\n",
            "           Conv2d-43           [-1, 64, 64, 64]          36,864\n",
            "      BatchNorm2d-44           [-1, 64, 64, 64]             128\n",
            "             ReLU-45           [-1, 64, 64, 64]               0\n",
            "       BasicBlock-46           [-1, 64, 64, 64]               0\n",
            "           Conv2d-47           [-1, 64, 64, 64]          36,864\n",
            "      BatchNorm2d-48           [-1, 64, 64, 64]             128\n",
            "             ReLU-49           [-1, 64, 64, 64]               0\n",
            "           Conv2d-50           [-1, 64, 64, 64]          36,864\n",
            "      BatchNorm2d-51           [-1, 64, 64, 64]             128\n",
            "             ReLU-52           [-1, 64, 64, 64]               0\n",
            "       BasicBlock-53           [-1, 64, 64, 64]               0\n",
            "           Conv2d-54           [-1, 64, 64, 64]          36,864\n",
            "      BatchNorm2d-55           [-1, 64, 64, 64]             128\n",
            "             ReLU-56           [-1, 64, 64, 64]               0\n",
            "           Conv2d-57           [-1, 64, 64, 64]          36,864\n",
            "      BatchNorm2d-58           [-1, 64, 64, 64]             128\n",
            "             ReLU-59           [-1, 64, 64, 64]               0\n",
            "       BasicBlock-60           [-1, 64, 64, 64]               0\n",
            "           Conv2d-61          [-1, 128, 32, 32]          73,728\n",
            "      BatchNorm2d-62          [-1, 128, 32, 32]             256\n",
            "             ReLU-63          [-1, 128, 32, 32]               0\n",
            "           Conv2d-64          [-1, 128, 32, 32]         147,456\n",
            "      BatchNorm2d-65          [-1, 128, 32, 32]             256\n",
            "           Conv2d-66          [-1, 128, 32, 32]           8,192\n",
            "      BatchNorm2d-67          [-1, 128, 32, 32]             256\n",
            "             ReLU-68          [-1, 128, 32, 32]               0\n",
            "       BasicBlock-69          [-1, 128, 32, 32]               0\n",
            "           Conv2d-70          [-1, 128, 32, 32]         147,456\n",
            "      BatchNorm2d-71          [-1, 128, 32, 32]             256\n",
            "             ReLU-72          [-1, 128, 32, 32]               0\n",
            "           Conv2d-73          [-1, 128, 32, 32]         147,456\n",
            "      BatchNorm2d-74          [-1, 128, 32, 32]             256\n",
            "             ReLU-75          [-1, 128, 32, 32]               0\n",
            "       BasicBlock-76          [-1, 128, 32, 32]               0\n",
            "           Conv2d-77          [-1, 128, 32, 32]         147,456\n",
            "      BatchNorm2d-78          [-1, 128, 32, 32]             256\n",
            "             ReLU-79          [-1, 128, 32, 32]               0\n",
            "           Conv2d-80          [-1, 128, 32, 32]         147,456\n",
            "      BatchNorm2d-81          [-1, 128, 32, 32]             256\n",
            "             ReLU-82          [-1, 128, 32, 32]               0\n",
            "       BasicBlock-83          [-1, 128, 32, 32]               0\n",
            "           Conv2d-84          [-1, 128, 32, 32]         147,456\n",
            "      BatchNorm2d-85          [-1, 128, 32, 32]             256\n",
            "             ReLU-86          [-1, 128, 32, 32]               0\n",
            "           Conv2d-87          [-1, 128, 32, 32]         147,456\n",
            "      BatchNorm2d-88          [-1, 128, 32, 32]             256\n",
            "             ReLU-89          [-1, 128, 32, 32]               0\n",
            "       BasicBlock-90          [-1, 128, 32, 32]               0\n",
            "           Conv2d-91          [-1, 256, 16, 16]         294,912\n",
            "      BatchNorm2d-92          [-1, 256, 16, 16]             512\n",
            "             ReLU-93          [-1, 256, 16, 16]               0\n",
            "           Conv2d-94          [-1, 256, 16, 16]         589,824\n",
            "      BatchNorm2d-95          [-1, 256, 16, 16]             512\n",
            "           Conv2d-96          [-1, 256, 16, 16]          32,768\n",
            "      BatchNorm2d-97          [-1, 256, 16, 16]             512\n",
            "             ReLU-98          [-1, 256, 16, 16]               0\n",
            "       BasicBlock-99          [-1, 256, 16, 16]               0\n",
            "          Conv2d-100          [-1, 256, 16, 16]         589,824\n",
            "     BatchNorm2d-101          [-1, 256, 16, 16]             512\n",
            "            ReLU-102          [-1, 256, 16, 16]               0\n",
            "          Conv2d-103          [-1, 256, 16, 16]         589,824\n",
            "     BatchNorm2d-104          [-1, 256, 16, 16]             512\n",
            "            ReLU-105          [-1, 256, 16, 16]               0\n",
            "      BasicBlock-106          [-1, 256, 16, 16]               0\n",
            "          Conv2d-107          [-1, 256, 16, 16]         589,824\n",
            "     BatchNorm2d-108          [-1, 256, 16, 16]             512\n",
            "            ReLU-109          [-1, 256, 16, 16]               0\n",
            "          Conv2d-110          [-1, 256, 16, 16]         589,824\n",
            "     BatchNorm2d-111          [-1, 256, 16, 16]             512\n",
            "            ReLU-112          [-1, 256, 16, 16]               0\n",
            "      BasicBlock-113          [-1, 256, 16, 16]               0\n",
            "          Conv2d-114          [-1, 256, 16, 16]         589,824\n",
            "     BatchNorm2d-115          [-1, 256, 16, 16]             512\n",
            "            ReLU-116          [-1, 256, 16, 16]               0\n",
            "          Conv2d-117          [-1, 256, 16, 16]         589,824\n",
            "     BatchNorm2d-118          [-1, 256, 16, 16]             512\n",
            "            ReLU-119          [-1, 256, 16, 16]               0\n",
            "      BasicBlock-120          [-1, 256, 16, 16]               0\n",
            "          Conv2d-121          [-1, 256, 16, 16]         589,824\n",
            "     BatchNorm2d-122          [-1, 256, 16, 16]             512\n",
            "            ReLU-123          [-1, 256, 16, 16]               0\n",
            "          Conv2d-124          [-1, 256, 16, 16]         589,824\n",
            "     BatchNorm2d-125          [-1, 256, 16, 16]             512\n",
            "            ReLU-126          [-1, 256, 16, 16]               0\n",
            "      BasicBlock-127          [-1, 256, 16, 16]               0\n",
            "          Conv2d-128          [-1, 256, 16, 16]         589,824\n",
            "     BatchNorm2d-129          [-1, 256, 16, 16]             512\n",
            "            ReLU-130          [-1, 256, 16, 16]               0\n",
            "          Conv2d-131          [-1, 256, 16, 16]         589,824\n",
            "     BatchNorm2d-132          [-1, 256, 16, 16]             512\n",
            "            ReLU-133          [-1, 256, 16, 16]               0\n",
            "      BasicBlock-134          [-1, 256, 16, 16]               0\n",
            "          Conv2d-135            [-1, 512, 8, 8]       1,179,648\n",
            "     BatchNorm2d-136            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-137            [-1, 512, 8, 8]               0\n",
            "          Conv2d-138            [-1, 512, 8, 8]       2,359,296\n",
            "     BatchNorm2d-139            [-1, 512, 8, 8]           1,024\n",
            "          Conv2d-140            [-1, 512, 8, 8]         131,072\n",
            "     BatchNorm2d-141            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-142            [-1, 512, 8, 8]               0\n",
            "      BasicBlock-143            [-1, 512, 8, 8]               0\n",
            "          Conv2d-144            [-1, 512, 8, 8]       2,359,296\n",
            "     BatchNorm2d-145            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-146            [-1, 512, 8, 8]               0\n",
            "          Conv2d-147            [-1, 512, 8, 8]       2,359,296\n",
            "     BatchNorm2d-148            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-149            [-1, 512, 8, 8]               0\n",
            "      BasicBlock-150            [-1, 512, 8, 8]               0\n",
            "          Conv2d-151            [-1, 512, 8, 8]       2,359,296\n",
            "     BatchNorm2d-152            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-153            [-1, 512, 8, 8]               0\n",
            "          Conv2d-154            [-1, 512, 8, 8]       2,359,296\n",
            "     BatchNorm2d-155            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-156            [-1, 512, 8, 8]               0\n",
            "      BasicBlock-157            [-1, 512, 8, 8]               0\n",
            "          Conv2d-158            [-1, 512, 8, 8]         524,800\n",
            " ConvTranspose2d-159          [-1, 512, 16, 16]       1,049,088\n",
            "            ReLU-160          [-1, 512, 16, 16]               0\n",
            "          Conv2d-161          [-1, 512, 16, 16]       2,359,808\n",
            "            ReLU-162          [-1, 512, 16, 16]               0\n",
            "          Conv2d-163          [-1, 512, 16, 16]       2,359,808\n",
            "            ReLU-164          [-1, 512, 16, 16]               0\n",
            " ConvTranspose2d-165          [-1, 256, 32, 32]         524,544\n",
            "            ReLU-166          [-1, 256, 32, 32]               0\n",
            "          Conv2d-167          [-1, 256, 32, 32]         590,080\n",
            "            ReLU-168          [-1, 256, 32, 32]               0\n",
            "          Conv2d-169          [-1, 256, 32, 32]         590,080\n",
            "            ReLU-170          [-1, 256, 32, 32]               0\n",
            " ConvTranspose2d-171          [-1, 128, 64, 64]         131,200\n",
            "            ReLU-172          [-1, 128, 64, 64]               0\n",
            "          Conv2d-173          [-1, 128, 64, 64]         147,584\n",
            "            ReLU-174          [-1, 128, 64, 64]               0\n",
            "          Conv2d-175          [-1, 128, 64, 64]         147,584\n",
            "            ReLU-176          [-1, 128, 64, 64]               0\n",
            " ConvTranspose2d-177         [-1, 64, 128, 128]          32,832\n",
            "            ReLU-178         [-1, 64, 128, 128]               0\n",
            "          Conv2d-179         [-1, 64, 128, 128]          36,928\n",
            "            ReLU-180         [-1, 64, 128, 128]               0\n",
            "          Conv2d-181         [-1, 64, 128, 128]          36,928\n",
            "            ReLU-182         [-1, 64, 128, 128]               0\n",
            " ConvTranspose2d-183         [-1, 32, 256, 256]           8,224\n",
            "            ReLU-184         [-1, 32, 256, 256]               0\n",
            "          Conv2d-185         [-1, 32, 256, 256]           9,248\n",
            "            ReLU-186         [-1, 32, 256, 256]               0\n",
            "          Conv2d-187         [-1, 32, 256, 256]           9,248\n",
            "            ReLU-188         [-1, 32, 256, 256]               0\n",
            "          Conv2d-189          [-1, 1, 256, 256]              33\n",
            "================================================================\n",
            "Total params: 34,558,305\n",
            "Trainable params: 13,273,633\n",
            "Non-trainable params: 21,284,672\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.25\n",
            "Forward/backward pass size (MB): 506.25\n",
            "Params size (MB): 131.83\n",
            "Estimated Total Size (MB): 638.33\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "67f4c620",
        "outputId": "cf495e8c-43f4-476c-c6fb-8d7ee5d3394d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2976229337.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0meval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Eval Loss: {eval_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
        "    eval_loss = evaluate(model, eval_loader, criterion, device)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Eval Loss: {eval_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KHSxPxI_0ru"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_batches(model, loader, device, num_batches=3, num_show_per_batch=5):\n",
        "    model.eval()\n",
        "    fig_rows = num_batches * num_show_per_batch\n",
        "    plt.figure(figsize=(12, fig_rows * 3))\n",
        "\n",
        "    row = 0\n",
        "    with torch.no_grad():\n",
        "        for b_idx, (L_batch, ab_batch) in enumerate(loader):\n",
        "            if b_idx >= num_batches:\n",
        "                break\n",
        "\n",
        "            L_batch, ab_batch = L_batch.to(device), ab_batch.to(device)\n",
        "            pred_ab_batch = model(L_batch)\n",
        "\n",
        "            # how many images to show from this batch\n",
        "            n = min(num_show_per_batch, L_batch.size(0))\n",
        "            for i in range(n):\n",
        "                L = L_batch[i]\n",
        "                ab_gt = ab_batch[i]\n",
        "                ab_pred = pred_ab_batch[i]\n",
        "\n",
        "                rgb_gt = lab_to_rgb(L, ab_gt)\n",
        "                rgb_pred = lab_to_rgb(L, ab_pred)\n",
        "                gray = L[0].cpu().numpy()\n",
        "\n",
        "                plt.subplot(fig_rows, 3, row*3 + 1)\n",
        "                plt.imshow(gray, cmap='gray')\n",
        "                plt.title(f\"Batch {b_idx} – L\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                plt.subplot(fig_rows, 3, row*3 + 2)\n",
        "                plt.imshow(rgb_gt)\n",
        "                plt.title(\"Ground Truth\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                plt.subplot(fig_rows, 3, row*3 + 3)\n",
        "                plt.imshow(rgb_pred)\n",
        "                plt.title(\"Prediction\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                row += 1\n",
        "\n",
        "    plt.tight_layout(pad=1.5)\n",
        "    plt.show()\n",
        "\n",
        "# ==== Example call ====\n",
        "visualize_batches(model, eval_loader, device, num_batches=5, num_show_per_batch=10)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}